{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ec2cef9-d754-4112-be56-b6e11f44286b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-15 12:40:06.208134: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/trannhi/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Only the TensorFlow backend supports string inputs.\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow.data as tf_data\n",
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f88b5e-243d-4cb3-bccd-e5973c771bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Newsgroup20 data\n",
    "data_path = keras.utils.get_file(\n",
    "    \"news20.tar.gz\",\n",
    "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
    "    untar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc613ca8-e042-4e88-8161-1424499345bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories: 20\n",
      "Directory names: ['talk.politics.mideast', 'rec.autos', 'comp.sys.mac.hardware', 'alt.atheism', 'rec.sport.baseball', 'comp.os.ms-windows.misc', 'rec.sport.hockey', 'sci.crypt', 'sci.med', 'talk.politics.misc', 'rec.motorcycles', 'comp.windows.x', 'comp.graphics', 'comp.sys.ibm.pc.hardware', 'sci.electronics', 'talk.politics.guns', 'sci.space', 'soc.religion.christian', 'misc.forsale', 'talk.religion.misc']\n",
      "Number of files in comp.graphics: 1000\n",
      "Some example filenames: ['38254', '38402', '38630', '38865', '38891']\n"
     ]
    }
   ],
   "source": [
    "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
    "dirnames = os.listdir(data_dir)\n",
    "print(\"Number of directories:\", len(dirnames))\n",
    "print(\"Directory names:\", dirnames)\n",
    "\n",
    "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
    "print(\"Number of files in comp.graphics:\", len(fnames))\n",
    "print(\"Some example filenames:\", fnames[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea28d5db-5095-4a81-ad8f-73d9a11157fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newsgroups: comp.graphics\n",
      "Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!agate!dog.ee.lbl.gov!network.ucsd.edu!usc!rpi!nason110.its.rpi.edu!mabusj\n",
      "From: mabusj@nason110.its.rpi.edu (Jasen M. Mabus)\n",
      "Subject: Looking for Brain in CAD\n",
      "Message-ID: <c285m+p@rpi.edu>\n",
      "Nntp-Posting-Host: nason110.its.rpi.edu\n",
      "Reply-To: mabusj@rpi.edu\n",
      "Organization: Rensselaer Polytechnic Institute, Troy, NY.\n",
      "Date: Thu, 29 Apr 1993 23:27:20 GMT\n",
      "Lines: 7\n",
      "\n",
      "Jasen Mabus\n",
      "RPI student\n",
      "\n",
      "\tI am looking for a hman brain in any CAD (.dxf,.cad,.iges,.cgm,etc.) or picture (.gif,.jpg,.ras,etc.) format for an animation demonstration. If any has or knows of a location please reply by e-mail to mabusj@rpi.edu.\n",
      "\n",
      "Thank you in advance,\n",
      "Jasen Mabus  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(open(data_dir / \"comp.graphics\" / \"38987\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c8e31a-dd93-4a67-86a6-c54bef5c2eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing alt.atheism, 1000 files found\n",
      "Processing comp.graphics, 1000 files found\n",
      "Processing comp.os.ms-windows.misc, 1000 files found\n",
      "Processing comp.sys.ibm.pc.hardware, 1000 files found\n",
      "Processing comp.sys.mac.hardware, 1000 files found\n",
      "Processing comp.windows.x, 1000 files found\n",
      "Processing misc.forsale, 1000 files found\n",
      "Processing rec.autos, 1000 files found\n",
      "Processing rec.motorcycles, 1000 files found\n",
      "Processing rec.sport.baseball, 1000 files found\n",
      "Processing rec.sport.hockey, 1000 files found\n",
      "Processing sci.crypt, 1000 files found\n",
      "Processing sci.electronics, 1000 files found\n",
      "Processing sci.med, 1000 files found\n",
      "Processing sci.space, 1000 files found\n",
      "Processing soc.religion.christian, 997 files found\n",
      "Processing talk.politics.guns, 1000 files found\n",
      "Processing talk.politics.mideast, 1000 files found\n",
      "Processing talk.politics.misc, 1000 files found\n",
      "Processing talk.religion.misc, 1000 files found\n",
      "Classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Number of samples: 19997\n"
     ]
    }
   ],
   "source": [
    "# remove headers from files\n",
    "samples = []\n",
    "labels = []\n",
    "class_names = []\n",
    "class_index = 0\n",
    "for dirname in sorted(os.listdir(data_dir)):\n",
    "    class_names.append(dirname)\n",
    "    dirpath = data_dir / dirname\n",
    "    fnames = os.listdir(dirpath)\n",
    "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
    "    for fname in fnames:\n",
    "        fpath = dirpath / fname\n",
    "        f = open(fpath, encoding=\"latin-1\")\n",
    "        content = f.read()\n",
    "        lines = content.split(\"\\n\")\n",
    "        lines = lines[10:]\n",
    "        content = \"\\n\".join(lines)\n",
    "        samples.append(content)\n",
    "        labels.append(class_index)\n",
    "    class_index += 1\n",
    "\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Number of samples:\", len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a60e765-c246-4a9f-985f-36356ae2408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "train_samples, val_samples, train_labels, val_labels = sklearn.model_selection.train_test_split(samples, labels, test_size = 0.2, random_state = 1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "816668bc-c5f7-47db-bc49-a50de40820b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-15 12:53:43.667074: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [15997]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "# Use TextVectorization to index the vocabulary found in the dataset\n",
    "# Only consider the top 20,000 words and pad sequences to be 200 tokens long\n",
    "vectorizer = layers.TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "text_ds = tf_data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be0e6126-4f45-42be-bfb3-ae6e7529a156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'to', 'of']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the computed vocab\n",
    "vectorizer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a15d18-3ff6-4849-9155-7128c6a9405c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 3505, 1737,   15,    2, 5984])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize a test sentence\n",
    "output = vectorizer([\"the cat sat on the mat\"])\n",
    "output.numpy()[0, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21738013-89c8-44e2-9ff0-814b370aeacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load the Glove Embeddings. Make a dict mapping words to their numpy vector representation\n",
    "path_to_glove_file = \"glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f912e3fa-4aa2-45ec-b3b2-b57adae2f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dict mapping words to their indices\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcd58738-a897-49f3-b5f5-7fe3e7ebfdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 17953 words (2047 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "603b1d45-a06e-471c-b562-c89c21cea468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained word embeddings matrix into an Embedding layer\n",
    "# Set trainable=False to keep the embeddings fixed\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39f4a172-0653-45fd-8fb5-e251a81bc71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, None, 100)         2000200   \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, None, 128)         64128     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, None, 128)        0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, None, 128)        0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                2580      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,247,516\n",
      "Trainable params: 247,316\n",
      "Non-trainable params: 2,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f66a9be-7622-4188-88e7-add7e18007b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# convert list-of-strings data to NumPy arrays of integer indices\n",
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21732779-68e6-44ed-a737-5854b773b005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "125/125 [==============================] - 20s 151ms/step - loss: 2.7225 - accuracy: 0.1240 - val_loss: 2.0774 - val_accuracy: 0.2850\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 1.9887 - accuracy: 0.3063 - val_loss: 1.5785 - val_accuracy: 0.4435\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 1.5579 - accuracy: 0.4535 - val_loss: 1.3704 - val_accuracy: 0.5185\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 1.2891 - accuracy: 0.5503 - val_loss: 1.2496 - val_accuracy: 0.5720\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 1.1288 - accuracy: 0.6095 - val_loss: 1.1009 - val_accuracy: 0.6230\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.9872 - accuracy: 0.6604 - val_loss: 1.0996 - val_accuracy: 0.6258\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.8747 - accuracy: 0.6973 - val_loss: 1.1149 - val_accuracy: 0.6208\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.7832 - accuracy: 0.7274 - val_loss: 1.0990 - val_accuracy: 0.6543\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 22s 174ms/step - loss: 0.6838 - accuracy: 0.7611 - val_loss: 1.0172 - val_accuracy: 0.6845\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 24s 191ms/step - loss: 0.6048 - accuracy: 0.7892 - val_loss: 1.0065 - val_accuracy: 0.6883\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 0.5241 - accuracy: 0.8178 - val_loss: 1.0441 - val_accuracy: 0.6938\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 0.4672 - accuracy: 0.8378 - val_loss: 1.1239 - val_accuracy: 0.6680\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.3997 - accuracy: 0.8583 - val_loss: 1.0378 - val_accuracy: 0.7092\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.3461 - accuracy: 0.8802 - val_loss: 1.0941 - val_accuracy: 0.7085\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.3101 - accuracy: 0.8931 - val_loss: 1.2287 - val_accuracy: 0.6930\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.2771 - accuracy: 0.9048 - val_loss: 1.1708 - val_accuracy: 0.7100\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.2529 - accuracy: 0.9133 - val_loss: 1.3008 - val_accuracy: 0.7040\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 22s 174ms/step - loss: 0.2198 - accuracy: 0.9239 - val_loss: 1.3271 - val_accuracy: 0.7067\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.2052 - accuracy: 0.9300 - val_loss: 1.3826 - val_accuracy: 0.7053\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 22s 177ms/step - loss: 0.1869 - accuracy: 0.9373 - val_loss: 2.2762 - val_accuracy: 0.6140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16ed16df0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and train the model\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e70379a0-6176-44ac-8430-d8743e036d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "def predict_using_model(model, phrase, class_names):\n",
    "  string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "  x = vectorizer(string_input)\n",
    "  preds = model(x)\n",
    "  end_to_end_model = keras.Model(string_input, preds)\n",
    "\n",
    "  probabilities = end_to_end_model.predict(\n",
    "    [[phrase]]\n",
    "  )\n",
    "  high = np.argmax(probabilities)\n",
    "  return class_names[high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0f6e446-b523-48b4-a106-c8b3c2e4e740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 256ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'comp.graphics'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_using_model(model, 'this message is about computer graphics and 3D modeling', class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb6f36f1-0d7a-44ce-95f4-2a239ed2d34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 233ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'talk.politics.guns'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_using_model(model, 'this message is about gun policies', class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aaf08b03-fa9b-4297-af7d-7927844f9ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x130c1a430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 221ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'rec.sport.baseball'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_using_model(model, 'this message is about baseball', class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8241323-571e-4a2c-b593-41890c9f2f70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
