{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f578d760-de16-4b8d-b0f4-9f9181141d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vista\n",
    "from vista.utils import logging\n",
    "logging.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feea8a89-4ce6-4084-8e79-8dff22e6adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Access documentation for VISTA\n",
    "### Run ?vista.<[name of module or function]>\n",
    "?vista.Display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d464859-338f-42d9-bad3-22575332a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the data for vista (auto-skip if already downloaded)\n",
    "!wget -nc -q --show-progress https://www.dropbox.com/s/62pao4mipyzk3xu/vista_traces.zip\n",
    "print(\"Unzipping data...\")\n",
    "!unzip -o -q vista_traces.zip\n",
    "print(\"Done downloading and unzipping data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4ef0d0-fa51-470c-a49a-bde33b383d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "trace_root = \"./vista_traces\"\n",
    "trace_path = [\n",
    "    \"20210726-154641_lexus_devens_center\", \n",
    "    \"20210726-155941_lexus_devens_center_reverse\", \n",
    "    \"20210726-184624_lexus_devens_center\", \n",
    "    \"20210726-184956_lexus_devens_center_reverse\", \n",
    "]\n",
    "trace_path = [os.path.join(trace_root, p) for p in trace_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a93cc1-0c7d-49fe-baaa-55f3c56fb012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a virtual world with VISTA, the world is defined by a series of data traces\n",
    "world = vista.World(trace_path, trace_config={'road_width': 4})\n",
    "\n",
    "# Create a car in our virtual world. The car will be able to step and take different \n",
    "#   control actions. As the car moves, its sensors will simulate any changes it environment\n",
    "car = world.spawn_agent(\n",
    "    config={\n",
    "        'length': 5.,\n",
    "        'width': 2.,\n",
    "        'wheel_base': 2.78,\n",
    "        'steering_ratio': 14.7,\n",
    "        'lookahead_road': True\n",
    "    })\n",
    "\n",
    "# Create a camera on the car for synthesizing the sensor data that we can use to train with! \n",
    "camera = car.spawn_camera(config={'size': (200, 320)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d03bd-da0a-4ec0-9d96-9379fd5b0608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a rendering display so we can visualize the simulated car camera stream and also \n",
    "#   get see its physical location with respect to the road in its environment. \n",
    "display = vista.Display(world, display_config={\"gui_scale\": 2, \"vis_full_frame\": False})\n",
    "!ffmpeg -version\n",
    "# Define a simple helper function that allows us to reset VISTA and the rendering display\n",
    "def vista_reset():\n",
    "    world.reset()\n",
    "    display.reset()\n",
    "vista_reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736879c4-71be-4f76-9f37-3f09339375db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define a step function, to allow our virtual agent to step \n",
    "# with a given control command through the environment \n",
    "# agent can act with a desired curvature (turning radius, like steering angle)\n",
    "# and desired speed. if either is not provided then this step function will \n",
    "# use whatever the human executed at that time in the real data.\n",
    "\n",
    "def vista_step(curvature=None, speed=None):\n",
    "    # Arguments:\n",
    "    #   curvature: curvature to step with\n",
    "    #   speed: speed to step with\n",
    "    if curvature is None: \n",
    "        curvature = car.trace.f_curvature(car.timestamp)\n",
    "    if speed is None: \n",
    "        speed = car.trace.f_speed(car.timestamp)\n",
    "    \n",
    "    car.step_dynamics(action=np.array([curvature, speed]), dt=1/15.)\n",
    "    car.step_sensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e6245-148e-49fc-940e-d85e49d228a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os, subprocess, cv2\n",
    "\n",
    "# Create a simple helper class that will assist us in storing videos of the render\n",
    "class VideoStream():\n",
    "    def __init__(self):\n",
    "        self.tmp = \"./tmp\"\n",
    "        if os.path.exists(self.tmp) and os.path.isdir(self.tmp):\n",
    "            shutil.rmtree(self.tmp)\n",
    "        os.mkdir(self.tmp)\n",
    "    def write(self, image, index):\n",
    "        cv2.imwrite(os.path.join(self.tmp, f\"{index:04}.png\"), image)\n",
    "    def save(self, fname):\n",
    "        cmd = f\"/usr/local/bin/ffmpeg -f image2 -i {self.tmp}/%04d.png -crf 0 -y {fname}\"\n",
    "        subprocess.call(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b644cfc-a37f-4b49-96a9-769073e8b3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Render and inspect a human trace ##\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import mitdeeplearning as mdl\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "from IPython.display import HTML\n",
    "\n",
    "vista_reset()\n",
    "stream = VideoStream()\n",
    "\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    vista_step()\n",
    "    \n",
    "    # Render and save the display\n",
    "    vis_img = display.render()\n",
    "    stream.write(vis_img[:, :, ::-1], index=i)\n",
    "    if car.done: \n",
    "        break\n",
    "\n",
    "print(\"Saving trajectory of human following...\")\n",
    "stream.save(\"human_follow.mp4\") \n",
    "# Play the video\n",
    "mdl.lab3_old.play_video(\"human_follow.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c804f6e-ceb9-4b88-a222-53bbc6b7ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define terminal states and crashing conditions ##\n",
    "\n",
    "def check_out_of_lane(car):\n",
    "    distance_from_center = np.abs(car.relative_state.x)\n",
    "    road_width = car.trace.road_width \n",
    "    half_road_width = road_width / 2\n",
    "    return distance_from_center > half_road_width\n",
    "\n",
    "def check_exceed_max_rot(car):\n",
    "    maximal_rotation = np.pi / 10.\n",
    "    current_rotation = np.abs(car.relative_state.yaw)\n",
    "    return current_rotation > maximal_rotation\n",
    "\n",
    "def check_crash(car): \n",
    "    return check_out_of_lane(car) or check_exceed_max_rot(car) or car.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab9d20-c11d-42fb-9189-cf7d1ae88be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Behavior with random control policy ##\n",
    "\n",
    "i = 0\n",
    "num_crashes = 5\n",
    "stream = VideoStream()\n",
    "\n",
    "for _ in range(num_crashes):\n",
    "    vista_reset()\n",
    "    \n",
    "    while not check_crash(car):\n",
    "\n",
    "        # Sample a random curvature (between +/- 1/3), keep speed constant\n",
    "        curvature = np.random.uniform(-1/3, 1/3)\n",
    "\n",
    "        # Step the simulated car with the same action\n",
    "        vista_step(curvature=curvature)\n",
    "\n",
    "        # Render and save the display\n",
    "        vis_img = display.render()\n",
    "        stream.write(vis_img[:, :, ::-1], index=i)\n",
    "        i += 1\n",
    "    \n",
    "    print(f\"Car crashed on step {i}\")\n",
    "    for _ in range(5):\n",
    "        stream.write(vis_img[:, :, ::-1], index=i)\n",
    "        i += 1\n",
    "\n",
    "print(\"Saving trajectory with random policy...\")\n",
    "stream.save(\"random_policy.mp4\")\n",
    "mdl.lab3_old.play_video(\"random_policy.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227d0a1-0fbe-49c9-9269-5a5da3bb152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Reset the simulation environment\n",
    "vista_reset()\n",
    "\n",
    "# Get the raw sensor observations\n",
    "full_obs = car.observations[camera.name]\n",
    "\n",
    "# Convert the image from BGR (OpenCV default) to RGB\n",
    "full_obs_rgb = cv2.cvtColor(full_obs, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(full_obs_rgb)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c9ebce-1ba4-4847-a210-a0feb546a393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the region of interest (ROI) from camera parameters\n",
    "region_of_interest = camera.camera_param.get_roi()\n",
    "i1, j1, i2, j2 = region_of_interest\n",
    "\n",
    "# Crop the image to the ROI\n",
    "cropped_obs = full_obs[i1:i2, j1:j2]\n",
    "\n",
    "# Convert the cropped image from BGR (OpenCV default) to RGB (used by matplotlib)\n",
    "cropped_obs_rgb = cv2.cvtColor(cropped_obs, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the cropped image using matplotlib\n",
    "plt.imshow(cropped_obs_rgb)\n",
    "plt.axis('off')  # Hide axes for a cleaner look\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9762024-f942-4804-b6c5-6ec2f6fd40df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data preprocessing functions ##\n",
    "\n",
    "def preprocess(full_obs):\n",
    "    # Extract ROI\n",
    "    i1, j1, i2, j2 = camera.camera_param.get_roi()\n",
    "    obs = full_obs[i1:i2, j1:j2]\n",
    "    \n",
    "    # Rescale to [0, 1]\n",
    "    obs = obs / 255.\n",
    "    return obs\n",
    "\n",
    "def grab_and_preprocess_obs(car):\n",
    "    full_obs = car.observations[camera.name]\n",
    "    obs = preprocess(full_obs)\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbed0735-d2fe-4b36-bf8f-7edf1e30ae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the self-driving agent ###\n",
    "# Note: we start with a template CNN architecture -- experiment away as you \n",
    "#   try to optimize your agent!\n",
    "\n",
    "# Functionally define layers for convenience\n",
    "# All convolutional layers will have ReLu activation\n",
    "import tensorflow as tf\n",
    "import functools\n",
    "act = tf.keras.activations.swish\n",
    "Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='valid', activation=act)\n",
    "Flatten = tf.keras.layers.Flatten\n",
    "Dense = tf.keras.layers.Dense\n",
    "\n",
    "# Defines a CNN for the self-driving agent\n",
    "def create_driving_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # Convolutional layers\n",
    "        # First, 32 5x5 filters and 2x2 stride\n",
    "        Conv2D(filters=32, kernel_size=5, strides=2),\n",
    "\n",
    "        # TODO: define convolutional layers with 48 5x5 filters and 2x2 stride\n",
    "        Conv2D(filters=48, kernel_size=5, strides=2),\n",
    "\n",
    "        # TODO: define two convolutional layers with 64 3x3 filters and 2x2 stride\n",
    "        Conv2D(filters=64, kernel_size=3, strides=2),\n",
    "        Conv2D(filters=64, kernel_size=3, strides=2),\n",
    "        \n",
    "        Flatten(),\n",
    "\n",
    "        # Fully connected layer and output\n",
    "        Dense(units=128, activation=act),\n",
    "        \n",
    "        # TODO: define the output dimension of the last Dense layer. \n",
    "        #    Pay attention to the space the agent needs to act in.\n",
    "        #    Remember that this model is outputing a distribution of *continuous* \n",
    "        #    actions, which take a different shape than discrete actions.\n",
    "        #    How many outputs should there be to define a distribution?'''\n",
    "\n",
    "        Dense(units=2, activation=None)\n",
    "\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "driving_model = create_driving_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7bb59-ad31-4654-9fd9-5fb09df6ac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The self-driving learning algorithm ##\n",
    "import tensorflow_probability as tfp\n",
    "# hyperparameters\n",
    "max_curvature = 1/8. \n",
    "max_std = 0.1 \n",
    "\n",
    "def run_driving_model(image):\n",
    "    # Arguments:\n",
    "    #   image: an input image\n",
    "    # Returns:\n",
    "    #   pred_dist: predicted distribution of control actions \n",
    "    single_image_input = tf.rank(image) == 3  # missing 4th batch dimension\n",
    "    if single_image_input:\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "\n",
    "    '''TODO: get the prediction of the model given the current observation.'''    \n",
    "    distribution = driving_model(image)\n",
    "\n",
    "    mu, logsigma = tf.split(distribution, 2, axis=1)\n",
    "    mu = max_curvature * tf.tanh(mu) # conversion\n",
    "    sigma = max_std * tf.sigmoid(logsigma) + 0.005 # conversion\n",
    "    \n",
    "    '''TODO: define the predicted distribution of curvature, given the predicted\n",
    "    mean mu and standard deviation sigma. Use a Normal distribution as defined\n",
    "    in TF probability (hint: tfp.distributions)'''\n",
    "    pred_dist = tfp.distributions.Normal(mu, sigma)\n",
    "  \n",
    "    return pred_dist\n",
    "\n",
    "\n",
    "def compute_driving_loss(dist, actions, rewards):\n",
    "    # Arguments:\n",
    "    #   logits: network's predictions for actions to take\n",
    "    #   actions: the actions the agent took in an episode\n",
    "    #   rewards: the rewards the agent received in an episode\n",
    "    # Returns:\n",
    "    #   loss\n",
    "    '''TODO: complete the function call to compute the negative log probabilities\n",
    "    of the agent's actions.'''\n",
    "    neg_logprob = -1 * dist.log_prob(actions)\n",
    "\n",
    "    '''TODO: scale the negative log probability by the rewards.'''\n",
    "    loss = tf.reduce_mean(neg_logprob * rewards)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21944fb2-c186-4fc5-bf93-26ef77791ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Agent Memory ###\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self): \n",
    "        self.clear()\n",
    "\n",
    "  # Resets/restarts the memory buffer\n",
    "    def clear(self): \n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "  # Add observations, actions, rewards to memory\n",
    "    def add_to_memory(self, new_observation, new_action, new_reward): \n",
    "        self.observations.append(new_observation)\n",
    "        '''TODO: update the list of actions with new action'''\n",
    "        self.actions.append(new_action) # TODO\n",
    "        # ['''TODO''']\n",
    "        '''TODO: update the list of rewards with new reward'''\n",
    "        self.rewards.append(new_reward) # TODO\n",
    "        # ['''TODO''']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ba556-e44e-4f7b-aea9-97c6096ad05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reward function ###\n",
    "\n",
    "# Helper function that normalizes an np.array x\n",
    "def normalize(x):\n",
    "    x -= np.mean(x)\n",
    "    x /= np.std(x)\n",
    "    return x.astype(np.float32)\n",
    "\n",
    "# Compute normalized, discounted, cumulative rewards (i.e., return)\n",
    "# Arguments:\n",
    "#   rewards: reward at timesteps in episode\n",
    "#   gamma: discounting factor\n",
    "# Returns:\n",
    "#   normalized discounted reward\n",
    "def discount_rewards(rewards, gamma=0.95): \n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    R = 0\n",
    "    for t in reversed(range(0, len(rewards))):\n",
    "        # update the total discounted reward\n",
    "        R = R * gamma + rewards[t]\n",
    "        discounted_rewards[t] = R\n",
    "      \n",
    "    return normalize(discounted_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c2c0c6-b3e7-4ed1-8254-2b3375502e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training parameters and initialization ##\n",
    "## Re-run this cell to restart training from scratch ##\n",
    "\n",
    "''' TODO: Learning rate and optimizer '''\n",
    "learning_rate = 1e-4\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# instantiate driving agent\n",
    "vista_reset()\n",
    "driving_model = create_driving_model()\n",
    "# NOTE: the variable driving_model will be used in run_driving_model execution\n",
    "\n",
    "# to track our progress\n",
    "smoothed_reward = mdl.util.LossHistory(smoothing_factor=0.9)\n",
    "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Rewards')\n",
    "\n",
    "# instantiate Memory buffer\n",
    "memory = Memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2a00c-a82c-4ded-af92-d5535249b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training step (forward and backpropagation) ###\n",
    "\n",
    "def train_step(model, loss_function, optimizer, observations, actions, discounted_rewards, custom_fwd_fn=None):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward propagate through the agent network\n",
    "        if custom_fwd_fn is not None:\n",
    "            prediction = custom_fwd_fn(observations)\n",
    "        else: \n",
    "            prediction = model(observations)\n",
    "\n",
    "        '''TODO: call the compute_loss function to compute the loss'''\n",
    "        loss = loss_function(prediction, actions, discounted_rewards) # TODO\n",
    "        # loss = loss_function('''TODO''', '''TODO''', '''TODO''')\n",
    "\n",
    "    '''TODO: run backpropagation to minimize the loss using the tape.gradient method. \n",
    "             Unlike supervised learning, RL is *extremely* noisy, so you will benefit \n",
    "             from additionally clipping your gradients to avoid falling into \n",
    "             dangerous local minima. After computing your gradients try also clipping\n",
    "             by a global normalizer. Try different clipping values, usually clipping \n",
    "             between 0.5 and 5 provides reasonable results. '''\n",
    "    grads = tape.gradient(loss, model.trainable_variables) # TODO\n",
    "    # grads = tape.gradient('''TODO''', '''TODO''')\n",
    "    grads, _ = tf.clip_by_global_norm(grads, 2)\n",
    "    # grads, _ = tf.clip_by_global_norm(grads, '''TODO''')\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8ba7e-0397-453d-a460-25faf7368525",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Driving training! Main training block. ##\n",
    "## Note: stopping and restarting this cell will pick up training where you\n",
    "#        left off. To restart training you need to rerun the cell above as \n",
    "#        well (to re-initialize the model and optimizer)\n",
    "\n",
    "max_batch_size = 300\n",
    "max_reward = float('-inf') # keep track of the maximum reward acheived during training\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "for i_episode in range(500):\n",
    "\n",
    "    plotter.plot(smoothed_reward.get())\n",
    "    # Restart the environment\n",
    "    vista_reset()\n",
    "    memory.clear()\n",
    "    observation = grab_and_preprocess_obs(car)\n",
    "\n",
    "    while True:\n",
    "        # TODO: using the car's current observation compute the desired \n",
    "        #  action (curvature) distribution by feeding it into our \n",
    "        #  driving model (use the function you already built to do this!) '''\n",
    "        curvature_dist = run_driving_model(observation)\n",
    "        \n",
    "        # TODO: sample from the action *distribution* to decide how to step\n",
    "        #   the car in the environment. You may want to check the documentation\n",
    "        #   for tfp.distributions.Normal online. Remember that the sampled action\n",
    "        #   should be a single scalar value after this step.\n",
    "        curvature_action = curvature_dist.sample()[0,0]\n",
    "        \n",
    "        # Step the simulated car with the same action\n",
    "        vista_step(curvature_action)\n",
    "        observation = grab_and_preprocess_obs(car)\n",
    "               \n",
    "        # TODO: Compute the reward for this iteration. You define \n",
    "        #   the reward function for this policy, start with something \n",
    "        #   simple - for example, give a reward of 1 if the car did not \n",
    "        #   crash and a reward of 0 if it did crash.\n",
    "        reward = 1.0 if not check_crash(car) else 0.0\n",
    "        \n",
    "        # add to memory\n",
    "        memory.add_to_memory(observation, curvature_action, reward)\n",
    "        \n",
    "        # is the episode over? did you crash or do so well that you're done?\n",
    "        if reward == 0.0:\n",
    "            # determine total reward and keep a record of this\n",
    "            total_reward = sum(memory.rewards)\n",
    "            smoothed_reward.append(total_reward)\n",
    "            \n",
    "            # execute training step - remember we don't know anything about how the \n",
    "            #   agent is doing until it has crashed! if the training step is too large \n",
    "            #   we need to sample a mini-batch for this step.\n",
    "            batch_size = min(len(memory), max_batch_size)\n",
    "            i = np.random.choice(len(memory), batch_size, replace=False)\n",
    "            train_step(driving_model, compute_driving_loss, optimizer, \n",
    "                               observations=np.array(memory.observations)[i],\n",
    "                               actions=np.array(memory.actions)[i],\n",
    "                               discounted_rewards = discount_rewards(memory.rewards)[i], \n",
    "                               custom_fwd_fn=run_driving_model)            \n",
    "            # reset the memory\n",
    "            memory.clear()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a11e7-5b14-44b0-a948-76aa65a18978",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation block!##\n",
    "\n",
    "i_step = 0\n",
    "num_episodes = 5\n",
    "num_reset = 5\n",
    "stream = VideoStream()\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    # Restart the environment\n",
    "    vista_reset()\n",
    "    observation = grab_and_preprocess_obs(car)\n",
    "    \n",
    "    print(\"rolling out in env\")\n",
    "    episode_step = 0\n",
    "    while not check_crash(car) and episode_step < 100:\n",
    "        # using our observation, choose an action and take it in the environment\n",
    "        curvature_dist = run_driving_model(observation)\n",
    "        curvature = curvature_dist.mean()[0,0]\n",
    "\n",
    "        # Step the simulated car with the same action\n",
    "        vista_step(curvature)\n",
    "        observation = grab_and_preprocess_obs(car)\n",
    "\n",
    "        vis_img = display.render()\n",
    "        stream.write(vis_img[:, :, ::-1], index=i_step)\n",
    "        i_step += 1\n",
    "        episode_step += 1\n",
    "        \n",
    "    for _ in range(num_reset):\n",
    "        stream.write(np.zeros_like(vis_img), index=i_step)\n",
    "        i_step += 1\n",
    "        \n",
    "print(f\"Average reward: {(i_step - (num_reset*num_episodes)) / num_episodes}\")\n",
    "\n",
    "print(\"Saving trajectory with trained policy...\")\n",
    "stream.save(\"trained_policy.mp4\")\n",
    "mdl.lab3_old.play_video(\"trained_policy.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f0bcd-0dab-4811-b3fb-3d7ea2195682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09420e7-90ea-4f98-acaf-3c17912d5f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
